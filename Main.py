import mathimport randomimport matplotlib.pyplot as pltfrom collections import namedtuple, dequefrom itertools import countfrom classes import Env, ReplayMemory, DQNimport torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as F#HyperparamatersBATCH_SIZE = 32GAMMA = 0.999EPS_START = 1EPS_END = 0EPS_DECAY = 200TARGET_UPDATE = 10 NUM_EPISODES = 1000MEMORY = 10000# Initialize the environment and other variablesenv = Env()policy_net = DQN().to(device)target_net = DQN().to(device)optimizer = optim.RMSprop(policy_net.parameters())memory = ReplayMemory(MEMORY)steps_done = 0def select_action(state):    # Outputs a 1x2 tensor representing which action to take. Input is an integer representing the current state    global steps_done    # Calculate the current epsilon value and the threshold, if the random sample is greater than the threshold, we use the neural net to pick the optimal action, if it's less, we pick a random action    sample = random.random()    eps_threshold = EPS_END + (EPS_START - EPS_END) * \        math.exp(-1. * steps_done / EPS_DECAY)    steps_done += 1    if sample > eps_threshold:        with torch.no_grad():            # Put the state through the net, and take the max of the output, and then return the index at which it occurs, tells us which action is optimal            return policy_net(torch.tensor([[state]], dtype = torch.float)).max(1)[1].view(1,1)    else:        return torch.tensor([[random.randint(0,1)]], device=device)def optimize_model():    if len(memory) < BATCH_SIZE:        # Can't optimize the model until we have enough transitions stored        return    # Sample a batch of transitions from memory. Then unzip it so there are now a batch of arrays instead of an array of batches    transitions = memory.sample(BATCH_SIZE)    batch = Transition(*zip(*transitions))    # Setting up the proper batches    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])    state_batch = torch.cat(batch.state)    #print('state batch:', state_batch)    action_batch = torch.cat(batch.action)    #print('action batch:', action_batch)    reward_batch = torch.cat(batch.reward)    #print('reward batch:', reward_batch)    # What actions the neural net would've chosen to take    state_action_values = policy_net(state_batch).gather(1, action_batch)    # Use the target net to calculate the values of the next states    next_state_values = torch.zeros(BATCH_SIZE, device=device)    k = target_net(non_final_next_states).max(1)[0].detach()    next_state_values[non_final_mask] = k    # Compute the expected Q values using the Bellman equation    expected_state_action_values = (next_state_values * GAMMA) + reward_batch    # Compute Huber loss    criterion = nn.SmoothL1Loss()    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))    # Optimize the model    optimizer.zero_grad()    loss.backward()    for param in policy_net.parameters():        param.grad.data.clamp_(-1, 1)    optimizer.step()plt.figure(1)def plot():    # Plot the progress of the model with 20 episode averages    plt.xlabel('Episode')    plt.ylabel('Score')    plt.title('Results')    if len(DQN_rewards) < 20:        DQN_av.append(0)    else:        l = len(DQN_rewards)        DQN_av.append(sum(DQN_rewards[l-20:])/20)    plt.plot(DQN_av)    plt.pause(0.01)DQN_rewards = []DQN_av = []for i_episode in range(NUM_EPISODES):    # Episodic loop, start with a reset environment    env.reset()    state = env.observe()    for t in count():        # Loop for every action or timestep        # Choose an action from the state and then perform that action        action = select_action(state)        reward, next_state, done = env.perform_action(action)        # Convert to tensor before storing in memory        reward = torch.tensor([reward], device = device, dtype=torch.float)        next_state = (torch.tensor([[next_state]], device = device, dtype = torch.float)                      if next_state is not None else None)        state = torch.tensor([[state]], device=device, dtype = torch.float)        memory.push(state, action, next_state, reward)        state = next_state        optimize_model()                if done:            # Episode finished, so append the reward and break loop            DQN_rewards.append(reward[0])            if i_episode%1 == 0:                plot()            break    if i_episode % TARGET_UPDATE == 0:        # Update the target net to the exact parameters of the policy net        target_net.load_state_dict(policy_net.state_dict())print(f'Average reward for DQN is {sum(DQN_rewards)/len(DQN_rewards)}')# New loop with a different agent with a hand picked deterministic policy to test againstrewards = []for i_episode in range(NUM_EPISODES):        env.reset()    for t in count():        # Policy is to hit until the state is more than 13                if env.observe() <= 13:            action = 1        else:            action = 0                reward, next_state, done = env.perform_action(action)                if done:            rewards.append(reward)            breakprint(f'average reward for tuned policy is {sum(rewards)/len(rewards)}')# Loop for a different agent that just picks random actions to test againstrandrewards = []for i_episode in range(NUM_EPISODES):        env.reset()    for t in count():        action = random.randint(0,1)        reward, next_state, done = env.perform_action(action)        if done:            randrewards.append(reward)            breakprint(f'average reward for rand policy is {sum(randrewards)/len(randrewards)}')# Testing the net to see what it learned print(policy_net(torch.tensor([[8.2]], dtype=torch.float, device=device)))# print(target_net(torch.tensor([[10]], dtype=torch.float, device=device)))print(policy_net(torch.tensor([[10.72]], dtype=torch.float, device=device)))# print(target_net(torch.tensor([[5]], dtype=torch.float, device=device)))print(policy_net(torch.tensor([[21]], dtype=torch.float, device=device)))# print(target_net(torch.tensor([[20]], dtype=torch.float, device=device)))